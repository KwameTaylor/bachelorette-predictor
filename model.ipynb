{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling for the Bachelorette Predictor\n",
    "### Kwame V. Taylor\n",
    "\n",
    "I will use linear regression and machine learning to predict values of contestants' ```ElimWeek```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, TweedieRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrangle import acquire_data, join_dfs, drop_extra_cols\n",
    "from preprocessing import handle_dates_and_elims, train_validate_test\n",
    "from model import model_1, model_1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, join = acquire_data()\n",
    "df = join_dfs(df, join)\n",
    "df = drop_extra_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (147, 4) | Shape of validate: (64, 4) | Shape of test: (53, 4)\n"
     ]
    }
   ],
   "source": [
    "df = handle_dates_and_elims(df)\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test, train, validate, test = train_validate_test(df, 'ElimWeek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "**The goal is to produce a predictive model that outperforms the baseline in predicting the target value -- in this case, ```ElimWeek```.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Evaluate Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.727891156462585"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.median(y_train)\n",
    "np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Root Mean Square Error) of Baseline on train data:\n",
      " 4.16706\n",
      "RMSE (Root Mean Square Error) of Baseline on validate data:\n",
      " 4.18615\n"
     ]
    }
   ],
   "source": [
    "#baseline = y_train.median()\n",
    "baseline = y_train.mean()\n",
    "\n",
    "baseline_rmse_train = round(mean_squared_error(y_train, np.full(len(y_train), baseline))**1/2, 6)\n",
    "print('RMSE (Root Mean Square Error) of Baseline on train data:\\n', baseline_rmse_train)\n",
    "baseline_rmse_validate = round(mean_squared_error(y_validate, np.full(len(y_validate), baseline))**1/2, 6)\n",
    "print('RMSE (Root Mean Square Error) of Baseline on validate data:\\n', baseline_rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean performed better than median. So, my baseline will be ```3.727891156462585```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MVP I'll just do one model, if it beats the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Ordinary Least Squares (OLS) using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Season', 'One-on-One_Score', 'FirstDate'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for OLS using LinearRegression\n",
      "\n",
      "On train data:\n",
      " 1.130448 \n",
      "\n",
      " On validate data:\n",
      " 1.005982\n"
     ]
    }
   ],
   "source": [
    "# use all features except season\n",
    "\n",
    "X = X_train.drop(columns=['Season'])\n",
    "y = y_train\n",
    "\n",
    "X_v = X_validate.drop(columns=['Season'])\n",
    "y_v = y_validate\n",
    "\n",
    "lm_pred, lm_rmse, lm_pred_v, lm_rmse_v = model_1(X, y, X_v, y_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs better than the baseline. ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that I know Model 1 is the best performing, I will test it on the test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for OLS using LinearRegression\n",
      "\n",
      "On test data:\n",
      " 0.896826\n"
     ]
    }
   ],
   "source": [
    "# use all features except season\n",
    "\n",
    "X = X_test.drop(columns=['Season'])\n",
    "y = y_test\n",
    "\n",
    "lm_pred, lm_rmse = model_1_test(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks good!! Model 1 performs the best and beats the baseline.** ðŸ¥³"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On next iteration of this project I will save a dataframe (as .csv) that has the best model's predictions in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will put this code into functions in a ```model.py``` file, and transfer my findings to the main notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
